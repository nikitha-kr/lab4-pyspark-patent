{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI 4253 / 5253 - Lab #4 - Patent Problem with Spark RDD - SOLUTION\n",
    "<div>\n",
    " <h2> CSCI 4283 / 5253 \n",
    "  <IMG SRC=\"https://www.colorado.edu/cs/profiles/express/themes/cuspirit/logo.png\" WIDTH=50 ALIGN=\"right\"/> </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This [Spark cheatsheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf) is useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import numpy as np\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf=SparkConf().setAppName(\"Lab4-rdd\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using PySpark and RDD's on the https://coding.csel.io machines is slow -- most of the code is executed in Python and this is much less efficient than the java-based code using the PySpark dataframes. Be patient and trying using `.cache()` to cache the output of joins. You may want to start with a reduced set of data before running the full task. You can use the `sample()` method to extract just a sample of the data or use "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two RDD's are called \"rawCitations\" and \"rawPatents\" because you probably want to process them futher (e.g. convert them to integer types, etc). \n",
    "\n",
    "The `textFile` function returns data in strings. This should work fine for this lab.\n",
    "\n",
    "Other methods you use might return data in type `Byte`. If you haven't used Python `Byte` types before, google it. You can convert a value of `x` type byte into e.g. a UTF8 string using `x.decode('uft-8')`. Alternatively, you can use the `open` method of the gzip library to read in all the lines as UTF-8 strings like this:\n",
    "```\n",
    "import gzip\n",
    "with gzip.open('cite75_99.txt.gz', 'rt',encoding='utf-8') as f:\n",
    "    rddCitations = sc.parallelize( f.readlines() )\n",
    "```\n",
    "This is less efficient than using `textFile` because `textFile` would use the underlying HDFS or other file system to read the file across all the worker nodes while the using `gzip.open()...readlines()` will read all the data in the frontend and then distribute it to all the worker nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddCitations = sc.textFile(\"cite75_99.txt.gz\")\n",
    "rddPatents = sc.textFile(\"apat63_99.txt.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data looks like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"CITING\",\"CITED\"',\n",
       " '3858241,956203',\n",
       " '3858241,1324234',\n",
       " '3858241,3398406',\n",
       " '3858241,3557384']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddCitations.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"PATENT\",\"GYEAR\",\"GDATE\",\"APPYEAR\",\"COUNTRY\",\"POSTATE\",\"ASSIGNEE\",\"ASSCODE\",\"CLAIMS\",\"NCLASS\",\"CAT\",\"SUBCAT\",\"CMADE\",\"CRECEIVE\",\"RATIOCIT\",\"GENERAL\",\"ORIGINAL\",\"FWDAPLAG\",\"BCKGTLAG\",\"SELFCTUB\",\"SELFCTLB\",\"SECDUPBD\",\"SECDLWBD\"',\n",
       " '3070801,1963,1096,,\"BE\",\"\",,1,,269,6,69,,1,,0,,,,,,,',\n",
       " '3070802,1963,1096,,\"US\",\"TX\",,1,,2,6,63,,0,,,,,,,,,',\n",
       " '3070803,1963,1096,,\"US\",\"IL\",,1,,2,6,63,,9,,0.3704,,,,,,,',\n",
       " '3070804,1963,1096,,\"US\",\"OH\",,1,,2,6,63,,3,,0.6667,,,,,,,']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddPatents.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, they are a single string with multiple CSV's. You will need to convert these to (K,V) pairs, probably convert the keys to `int` and so on. You'll need to `filter` out the header string as well since there's no easy way to extract all the lines except the first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports & Simple Helpers for Cleaning and Saving CSV Data in Spark\n",
    "- parse_csv_line: Parse rows safely.\n",
    "- drop_header: Remove header.\n",
    "- to_int_or_none: Clean numbers.\n",
    "- Storage Level & persist: Save processed data in Spark so it can be reused quickly.\n",
    "### 2.Parsing and Cleaning Citations & Patents Data in Spark\n",
    "- Citations: keep only who cites who.\n",
    "- Patents: keep only ID, country, and state.\n",
    "- Both are cleaned and stored so Spark can reuse them fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from io import StringIO\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "def parse_csv_line(line: str):\n",
    "    return next(csv.reader(StringIO(line)))\n",
    "\n",
    "def drop_header(rdd):\n",
    "    header = rdd.first()\n",
    "    return rdd.filter(lambda x: x != header)\n",
    "\n",
    "def to_int_or_none(s: str):\n",
    "    try:\n",
    "        return int(s)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "citations_data = drop_header(rddCitations).map(parse_csv_line)\n",
    "citations = (\n",
    "    citations_data\n",
    "    .map(lambda f: (to_int_or_none(f[0]), to_int_or_none(f[1])))\n",
    "    .filter(lambda t: t[0] is not None and t[1] is not None)\n",
    "    .persist(StorageLevel.MEMORY_AND_DISK) \n",
    ")\n",
    "\n",
    "patents_data = drop_header(rddPatents).map(parse_csv_line)\n",
    "patents_min = (\n",
    "    patents_data\n",
    "    .map(lambda f: (to_int_or_none(f[0]),\n",
    "                    f[4] if len(f) > 4 else \"\",\n",
    "                    f[5] if len(f) > 5 else \"\"))\n",
    "    .filter(lambda t: t[0] is not None) \n",
    "    .persist(StorageLevel.MEMORY_AND_DISK)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. creating a clean address book that tells us which U.S. state each patent belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_by_patent = (\n",
    "    patents_min\n",
    "    .filter(lambda t: t[1] == \"US\" and t[2] != \"\")\n",
    "    .map(lambda t: (t[0], t[2]))  # (patent_id, state)\n",
    "    .persist(StorageLevel.MEMORY_AND_DISK)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. attaching the U.S. state information to both the citing and cited patents in each citation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_cited = citations.map(lambda t: (t[1], t[0]))  # (cited, citing)\n",
    "with_cited_state = by_cited.leftOuterJoin(state_by_patent).persist(StorageLevel.MEMORY_AND_DISK) #(cited, (citing, cited_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "step1 = with_cited_state.map(lambda x: (x[1][0], (x[0], x[1][1])))  # (citing, (cited, cited_state))\n",
    "with_both = step1.leftOuterJoin(state_by_patent).persist(StorageLevel.MEMORY_AND_DISK) # (citing, ((cited, cited_state), citing_state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.reshaping the citation data into a simple table (Cited, Cited State, Citing, Citing State) and print 20 rows to double-check it looks right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cited\tState\tCiting\tState\n",
      "2085758\tNone\t4138233\tNone\n",
      "1959374\tNone\t4138233\tNone\n",
      "3443362\tNone\t4138233\tNone\n",
      "1976214\tNone\t4138233\tNone\n",
      "3915672\tPA\t4138233\tNone\n",
      "2682313\tNone\t4138233\tNone\n",
      "3984215\tFL\t4138233\tNone\n",
      "3350849\tNone\t4138233\tNone\n",
      "2019485\tNone\t4138233\tNone\n",
      "3443361\tMD\t4138233\tNone\n",
      "2181767\tNone\t4138233\tNone\n",
      "4052177\tNone\t4138233\tNone\n",
      "2195431\tNone\t4138233\tNone\n",
      "2519618\tNone\t5415732\tNone\n",
      "4334962\tFL\t5415732\tNone\n",
      "3974022\tNone\t5415732\tNone\n",
      "3634128\tUT\t5415732\tNone\n",
      "3702807\tNone\t5415732\tNone\n",
      "1006197\tNone\t5415732\tNone\n",
      "1717927\tNone\t5415732\tNone\n"
     ]
    }
   ],
   "source": [
    "intermediate_rdd = with_both.map(lambda kv: (\n",
    "    kv[1][0][0],   # Cited\n",
    "    kv[1][0][1] if kv[1][0][1] is not None else None,  # Cited_State\n",
    "    kv[0],         # Citing\n",
    "    kv[1][1] if kv[1][1] is not None else None         # Citing_State\n",
    "))\n",
    "\n",
    "print(\"Cited\\tState\\tCiting\\tState\")\n",
    "for row in intermediate_rdd.take(20):\n",
    "    print(f\"{row[0]}\\t{row[1]}\\t{row[2]}\\t{row[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. counting, for each citing patent, how many of its citations point to patents in the same state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_state_counts = (\n",
    "    intermediate_rdd\n",
    "    .filter(lambda r: (r[1] is not None) and (r[3] is not None) and (r[1] == r[3]))\n",
    "    .map(lambda r: (r[2], 1))                # (Citing, 1)\n",
    "    .reduceByKey(lambda a, b: a + b)         # (Citing, same_state_cites)\n",
    "    .persist(StorageLevel.MEMORY_AND_DISK)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. creating a final table where every patent has its state and a count of same-state citations, with 0 for those that don’t have any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_to_state = patents_min.map(lambda t: (t[0], t[2]))  # (PATENT, POSTATE)\n",
    "augmented_min = (\n",
    "    patent_to_state.leftOuterJoin(same_state_counts)  # (PATENT, (POSTATE, count))\n",
    "    .map(lambda kv: (kv[0], kv[1][0], 0 if kv[1][1] is None else kv[1][1]))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. listing the top 10 patents that cite the most other patents from the same state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 patents(PATENT, POSTATE, same_state_cites):\n",
      "(5959466, 'CA', 125)\n",
      "(5983822, 'TX', 103)\n",
      "(6008204, 'CA', 100)\n",
      "(5952345, 'CA', 98)\n",
      "(5958954, 'CA', 96)\n",
      "(5998655, 'CA', 96)\n",
      "(5936426, 'CA', 94)\n",
      "(5739256, 'CA', 90)\n",
      "(5913855, 'CA', 90)\n",
      "(5925042, 'CA', 90)\n"
     ]
    }
   ],
   "source": [
    "top10 = augmented_min.takeOrdered(10, key=lambda x: (-x[2], x[0]))\n",
    "print(\"\\nTop 10 patents(PATENT, POSTATE, same_state_cites):\")\n",
    "for rec in top10:\n",
    "    print(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
